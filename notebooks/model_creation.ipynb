{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Choice\n",
    "\n",
    "##### Why Random Forest over other types?\n",
    "\n",
    "Random Forest was the best choice over linear and logistic regression because it can handle more complex patterns in the data. It captures non-linear relationships and interactions between features, which these regression models often miss. \n",
    "It's also better at managing outliers, noise, and categorical variables without a lot of preprocessing. \n",
    "Plus, Random Forest handles class imbalances well, which was a big issue in this dataset. \n",
    "Linear and logistic regression models struggled with this imbalance, but Random Forest managed it effectively, making it the clear winner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 48 candidates, totalling 144 fits\n",
      "Best Parameters: {'class_weight': 'balanced', 'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Accuracy: 0.9311700259262357\n",
      "Confusion Matrix:\n",
      "[[5378  285    0    5    8    7    1   27]\n",
      " [ 187 5351   14   57    1    0    2    0]\n",
      " [   0   41   30    0    0    0    0    0]\n",
      " [   8  129    0   58    0    0    0    0]\n",
      " [   8    0    0    0  198    0    0    0]\n",
      " [   3    4    0    0    2   24    0    2]\n",
      " [   9    7    0    0    2    1    5    0]\n",
      " [   8    2    0    0    3    0    0   90]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.94      0.95      5711\n",
      "           1       0.92      0.95      0.94      5612\n",
      "           2       0.68      0.42      0.52        71\n",
      "           3       0.48      0.30      0.37       195\n",
      "           4       0.93      0.96      0.94       206\n",
      "           5       0.75      0.69      0.72        35\n",
      "           6       0.62      0.21      0.31        24\n",
      "           7       0.76      0.87      0.81       103\n",
      "\n",
      "    accuracy                           0.93     11957\n",
      "   macro avg       0.76      0.67      0.69     11957\n",
      "weighted avg       0.93      0.93      0.93     11957\n",
      "\n",
      "Model saved to ../data/ran_for_model.joblib\n"
     ]
    }
   ],
   "source": [
    "# model_creation.py\n",
    "# from src.local import ARTIFACTS_PATH, DATA_PATH\n",
    "from joblib import dump\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from pandas import read_csv\n",
    "from os import path\n",
    "\n",
    "# Setting ARTIFACTS_PATH and DATA_PATH to work from the notebooks dir\n",
    "DATA_PATH = '../data/'\n",
    "ARTIFACTS_PATH = '../artifacts/'\n",
    "\n",
    "def load_and_preprocess_data(file_path, features, target):\n",
    "    \"\"\"\n",
    "    Load and preprocess the dataset.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the CSV file containing the data.\n",
    "        features (list): List of feature column names.\n",
    "        target (str): The name of the target column.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the preprocessed feature matrix (X), target vector (y), and label encoders.\n",
    "    \"\"\"\n",
    "    # Load the dataset\n",
    "    data = read_csv(file_path, low_memory=False)\n",
    "\n",
    "    # Handling missing values\n",
    "    imputer = SimpleImputer(strategy='most_frequent')\n",
    "    data[features] = imputer.fit_transform(data[features])\n",
    "\n",
    "    # Ensure the target column has no missing values\n",
    "    data[target] = data[target].fillna(data[target].mode()[0])\n",
    "\n",
    "    # Encode categorical features\n",
    "    label_encoders = {}\n",
    "    for col in features:\n",
    "        if data[col].dtype == 'object':\n",
    "            label_encoders[col] = LabelEncoder()\n",
    "            data[col] = label_encoders[col].fit_transform(data[col])\n",
    "\n",
    "    # Save the label encoders for later use\n",
    "    for col, le in label_encoders.items():\n",
    "        dump(le, path.join(ARTIFACTS_PATH,\n",
    "                    f'{col}_label_encoder.joblib'))\n",
    "\n",
    "    # Create a status mapping dynamically to include all unique statuses\n",
    "    unique_statuses = data[target].unique()\n",
    "    status_mapping = {label: idx for idx, label in enumerate(unique_statuses)}\n",
    "\n",
    "    # Map the target column using the dynamic status mapping\n",
    "    data[target] = data[target].map(status_mapping)\n",
    "\n",
    "    # Check for any NaN values in the mapped target column\n",
    "    if data[target].isnull().sum() > 0:\n",
    "        raise ValueError(\n",
    "            \"The 'status' column contains NaN values after mapping.\")\n",
    "\n",
    "    # Save the status mapping for later use\n",
    "    dump(status_mapping, path.join(\n",
    "        ARTIFACTS_PATH, 'status_mapping.joblib'))\n",
    "\n",
    "    X = data[features]\n",
    "    y = data[target]\n",
    "\n",
    "    return X, y, label_encoders\n",
    "\n",
    "\n",
    "def train_and_evaluate_model(X, y, param_grid):\n",
    "    \"\"\"\n",
    "    Train and evaluate the model using GridSearchCV for hyperparameter tuning.\n",
    "\n",
    "    Args:\n",
    "        X (DataFrame): The feature matrix.\n",
    "        y (Series): The target vector.\n",
    "        param_grid (dict): The parameter grid for hyperparameter tuning.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the trained model, scaler, best parameters, accuracy, confusion matrix, and classification report.\n",
    "    \"\"\"\n",
    "    # Splitting the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Standardizing numerical features\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Save the scaler for later use\n",
    "    dump(scaler, path.join(ARTIFACTS_PATH, 'scaler.joblib'))\n",
    "\n",
    "    # Initialize the Random Forest classifier\n",
    "    rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "    # Perform GridSearchCV for hyperparameter tuning\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=rf, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2, scoring='f1_macro')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Best parameters from the grid search\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    # Train the Random Forest classifier with the best parameters\n",
    "    best_rf = RandomForestClassifier(**best_params, random_state=42)\n",
    "    best_rf.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = best_rf.predict(X_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "    return best_rf, scaler, best_params, accuracy, conf_matrix, class_report\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to load data, train the model, and save the trained model.\"\"\"\n",
    "    file_path = path.join(DATA_PATH, 'combined_df.csv')\n",
    "    features = ['total_mass', 'span', 'period_mins', 'perigee_km', 'apogee_km',\n",
    "                'inclination', 'object_type']\n",
    "    target = 'status'\n",
    "\n",
    "    X, y, label_encoders = load_and_preprocess_data(\n",
    "        file_path, features, target)\n",
    "\n",
    "    # Define the reduced parameter grid for hyperparameter tuning\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5],\n",
    "        'min_samples_leaf': [1, 2],\n",
    "        'class_weight': [None, 'balanced']\n",
    "    }\n",
    "\n",
    "    best_rf, scaler, best_params, accuracy, conf_matrix, class_report = train_and_evaluate_model(\n",
    "        X, y, param_grid)\n",
    "\n",
    "    # Save the trained model to a file using joblib\n",
    "    model_path = path.join(ARTIFACTS_PATH, 'ran_for_model.joblib')\n",
    "    dump(best_rf, model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "    print(f\"Classification Report:\\n{class_report}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
