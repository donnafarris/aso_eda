{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Choice\n",
    "\n",
    "##### Why Random Forest over other types?\n",
    "\n",
    "Random Forest was the best choice over linear and logistic regression because it can handle more complex patterns in the data. It captures non-linear relationships and interactions between features, which these regression models often miss. \n",
    "It's also better at managing outliers, noise, and categorical variables without a lot of preprocessing. \n",
    "Plus, Random Forest handles class imbalances well, which was a big issue in this dataset. \n",
    "Linear and logistic regression models struggled with this imbalance, but Random Forest managed it effectively, making it the clear winner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 48 candidates, totalling 144 fits\n",
      "Best Parameters: {'class_weight': 'balanced', 'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Accuracy: 0.9311700259262357\n",
      "Confusion Matrix:\n",
      "[[5378  285    0    5    8    7    1   27]\n",
      " [ 187 5351   14   57    1    0    2    0]\n",
      " [   0   41   30    0    0    0    0    0]\n",
      " [   8  129    0   58    0    0    0    0]\n",
      " [   8    0    0    0  198    0    0    0]\n",
      " [   3    4    0    0    2   24    0    2]\n",
      " [   9    7    0    0    2    1    5    0]\n",
      " [   8    2    0    0    3    0    0   90]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.94      0.95      5711\n",
      "           1       0.92      0.95      0.94      5612\n",
      "           2       0.68      0.42      0.52        71\n",
      "           3       0.48      0.30      0.37       195\n",
      "           4       0.93      0.96      0.94       206\n",
      "           5       0.75      0.69      0.72        35\n",
      "           6       0.62      0.21      0.31        24\n",
      "           7       0.76      0.87      0.81       103\n",
      "\n",
      "    accuracy                           0.93     11957\n",
      "   macro avg       0.76      0.67      0.69     11957\n",
      "weighted avg       0.93      0.93      0.93     11957\n",
      "\n",
      "Model saved to ../artifacts/best_random_forest_orbital_parameters_model.joblib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import joblib\n",
    "\n",
    "# Load the dataset\n",
    "file_path = '../data/combined_df.csv'\n",
    "data = pd.read_csv(file_path, low_memory=False)\n",
    "\n",
    "# Selecting features and target\n",
    "features = [\n",
    "    'period_mins', 'perigee_km', 'apogee_km', 'inclination', \n",
    "    'object_type', 'object_owner'\n",
    "]\n",
    "target = 'status'\n",
    "\n",
    "# Handling missing values\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "data[features] = imputer.fit_transform(data[features])\n",
    "\n",
    "# Ensure the status column has no missing values\n",
    "data[target] = data[target].fillna(data[target].mode()[0])\n",
    "\n",
    "# Encode categorical features\n",
    "label_encoders = {}\n",
    "for col in features:\n",
    "    if data[col].dtype == 'object':\n",
    "        label_encoders[col] = LabelEncoder()\n",
    "        data[col] = label_encoders[col].fit_transform(data[col])\n",
    "\n",
    "# Identify unique values in the 'status' column\n",
    "unique_statuses = data[target].unique()\n",
    "\n",
    "# Create a status mapping dynamically to include all unique statuses\n",
    "status_mapping = {label: idx for idx, label in enumerate(unique_statuses)}\n",
    "\n",
    "# Map the status column using the dynamic status mapping\n",
    "data[target] = data[target].map(status_mapping)\n",
    "\n",
    "# Check for any NaN values in the mapped target column\n",
    "if data[target].isnull().sum() > 0:\n",
    "    raise ValueError(\"The 'status' column contains NaN values after mapping.\")\n",
    "\n",
    "# Save the status mapping for later use\n",
    "joblib.dump(status_mapping, '../artifacts/status_mapping.joblib')\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X = data[features]\n",
    "y = data[target]\n",
    "\n",
    "# Check for any NaN values in y\n",
    "if y.isnull().sum() > 0:\n",
    "    raise ValueError(\"Input y contains NaN values.\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardizing numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Save the scaler for later use\n",
    "joblib.dump(scaler, '../artifacts/scaler.joblib')\n",
    "\n",
    "# Define the reduced parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Perform GridSearchCV for hyperparameter tuning\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, \n",
    "                           cv=3, n_jobs=-1, verbose=2, scoring='f1_macro')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters from the grid search\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Train the Random Forest classifier with the best parameters\n",
    "best_rf = RandomForestClassifier(**best_params, random_state=42)\n",
    "best_rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Output the evaluation metrics\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n",
    "\n",
    "# Save the trained model to a file using joblib\n",
    "model_path = '../artifacts/best_random_forest_orbital_parameters_model.joblib'\n",
    "joblib.dump(best_rf, model_path)\n",
    "print(f\"Model saved to {model_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
